{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32e69c12",
   "metadata": {},
   "source": [
    "# MindLens-AI — 03: Explainability (RQ1)\n",
    "\n",
    "**RQ1:** Can explainable NLP models maintain high performance (≥80% accuracy) while providing meaningful, human-interpretable explanations aligned with mental health indicators?\n",
    "\n",
    "Tools: SHAP (global + local) and LIME (instance-level). Includes a quantitative Interpretability Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ec0ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import shap\n",
    "\n",
    "from src.model import load_model\n",
    "from src.explainability import (\n",
    "    explain_lime, explain_shap, shap_summary_plot,\n",
    "    compute_interpretability_score, MH_LEXICON,\n",
    ")\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Load artifacts\n",
    "model, vectorizer = load_model(\"../data/processed/model_artifacts.joblib\")\n",
    "split = joblib.load(\"../data/processed/test_split.joblib\")\n",
    "X_test = split[\"X_test\"]\n",
    "X_train = split[\"X_train\"]\n",
    "y_test = split[\"y_test\"]\n",
    "texts_test = split[\"texts_test\"]\n",
    "feature_names = split[\"feature_names\"]\n",
    "\n",
    "print(f\"Loaded model + {len(texts_test)} test samples ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6310ac",
   "metadata": {},
   "source": [
    "## 1. SHAP — Global Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683bae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP values\n",
    "shap_values, shap_explainer = explain_shap(model, X_train, X_test, feature_names)\n",
    "print(f\"SHAP values shape: {shap_values.shape}\")\n",
    "\n",
    "# Summary plot (top 20 global features)\n",
    "shap_summary_plot(shap_values, X_test, feature_names, max_display=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d15dae",
   "metadata": {},
   "source": [
    "## 2. SHAP — Force Plots (Individual Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76760d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 5 Risk + 5 No-Risk samples\n",
    "risk_idx = np.where(y_test == 1)[0][:5]\n",
    "safe_idx = np.where(y_test == 0)[0][:5]\n",
    "sample_indices = np.concatenate([risk_idx, safe_idx])\n",
    "\n",
    "for i in sample_indices:\n",
    "    label = \"RISK\" if y_test[i] == 1 else \"NO RISK\"\n",
    "    print(f\"\\n--- Sample {i} (True: {label}) ---\")\n",
    "    print(f\"Text: {texts_test[i][:150]}...\")\n",
    "    plt.figure()\n",
    "    shap.force_plot(\n",
    "        shap_explainer.expected_value,\n",
    "        shap_values[i],\n",
    "        feature_names=feature_names,\n",
    "        matplotlib=True,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b76dab",
   "metadata": {},
   "source": [
    "## 3. LIME — Instance-Level Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041c7327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME explanations for same 10 samples\n",
    "for i in sample_indices:\n",
    "    label = \"RISK\" if y_test[i] == 1 else \"NO RISK\"\n",
    "    text = texts_test[i]\n",
    "    print(f\"\\n--- LIME: Sample {i} (True: {label}) ---\")\n",
    "    print(f\"Text: {text[:150]}...\")\n",
    "\n",
    "    exp = explain_lime(model, vectorizer, text, num_features=10)\n",
    "    fig = exp.as_pyplot_figure()\n",
    "    plt.title(f\"LIME — Sample {i} ({label})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Show top words\n",
    "    print(\"Top features:\", exp.as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc4df8",
   "metadata": {},
   "source": [
    "## 4. Quantitative Interpretability Score\n",
    "\n",
    "**Interpretability Score** = (# of top-k SHAP features overlapping with mental health lexicon) / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb76d179",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mental Health Lexicon ({len(MH_LEXICON)} words):\")\n",
    "print(sorted(MH_LEXICON))\n",
    "\n",
    "# Compute at k=10 and k=20\n",
    "for k in [10, 20]:\n",
    "    result = compute_interpretability_score(shap_values, feature_names, MH_LEXICON, k=k)\n",
    "    print(f\"\\n--- Interpretability Score (k={k}) ---\")\n",
    "    print(f\"  Mean:  {result['mean_score']:.4f}\")\n",
    "    print(f\"  Std:   {result['std_score']:.4f}\")\n",
    "\n",
    "# Distribution plot\n",
    "result_k10 = compute_interpretability_score(shap_values, feature_names, MH_LEXICON, k=10)\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.hist(result_k10[\"per_sample_scores\"], bins=20, edgecolor=\"black\", alpha=0.7, color=\"#2196F3\")\n",
    "ax.axvline(result_k10[\"mean_score\"], color=\"red\", linestyle=\"--\", label=f\"Mean = {result_k10['mean_score']:.3f}\")\n",
    "ax.set_xlabel(\"Interpretability Score (k=10)\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Distribution of Interpretability Scores\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c166d59a",
   "metadata": {},
   "source": [
    "## 5. RQ1 Conclusion\n",
    "\n",
    "**H1:** Explainable models can achieve ≥80% accuracy without major performance loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a472236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import evaluate_single\n",
    "\n",
    "metrics = evaluate_single(model, X_test, y_test)\n",
    "accuracy = metrics[\"accuracy\"]\n",
    "interp_score = result_k10[\"mean_score\"]\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"RQ1 CONCLUSION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Accuracy:              {accuracy:.4f}  (target ≥ 0.80)\")\n",
    "print(f\"  F1 Score:              {metrics['f1']:.4f}\")\n",
    "print(f\"  Interpretability (k=10): {interp_score:.4f}\")\n",
    "print()\n",
    "\n",
    "if accuracy >= 0.80 and interp_score >= 0.15:\n",
    "    print(\"✓ H1 SUPPORTED: High accuracy with meaningful, interpretable explanations.\")\n",
    "else:\n",
    "    print(\"✗ H1 NOT SUPPORTED: Either accuracy < 80% or interpretability score too low.\")\n",
    "\n",
    "print(\"\\nExplainability notebook complete ✓\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
